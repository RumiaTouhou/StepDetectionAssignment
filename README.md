This repository contains a Python prototype that reads a short first-person video of a simple manual assembly task (assembling the handle of a glass lid), then it detects an ordered sequence of work steps and exports timestamps, logs, plots, and optional step-by-step instructions.

## Context and goal

The goal is to demonstrate a clear generalizable technical approach to multimodal activity recognition in a realistic setting, where signals are noisy and partial, and where an explainable design is useful for later deployment in instruction generation and XR scenarios.

## What the system does

**Input**
- A first-person video (`.mp4`) of a person assembling a pot lid handle.

**Outputs**
- A sequence of detected steps with start and end timestamps.
- CSV exports for per-frame features and per-step events.
- A JSON event log that can be used as input to an instruction generator.
- Optional instruction text, which can be generated by templates or by DeepSeek (optional).
- Plots for the step timeline and per-frame features.
- Optional annotated video with overlaid step labels (see limitations about audio).

## Pipeline overview

The system follows a simple and explainable pipeline:

1. **Perception (multimodal feature extraction)**  
   The program samples the video at a low frame rate (default 5 FPS), then it extracts:
   - vision motion features (optical flow, rotation index, center versus rim separation)
   - hand features (MediaPipe Hands, hand positions, speed, grip proxies, palm-normal rotation speed)
   - tool and handle heuristic scores (ROI-based)
   - audio features (RMS, onsets, low/high band onsets)

2. **Step inference (ordered context FSM)**  
   An ordered finite state machine (FSM) progresses through the expected procedure steps in a forward-only way, where it uses debounced signals, counters, and per-video adaptive thresholds to decide state transitions.

3. **Exports and visualization**  
   The system saves CSV and JSON outputs, then it saves plots, and it can also write an annotated video.

4. **Optional instruction generation (LLM or templates)**  
   The system converts the detected step sequence into a step-by-step instruction document, either by deterministic templates or by an optional DeepSeek API call.

## Repository structure

- `main.py`  
  Main entry point, runs the full pipeline and writes all outputs.

- `models.py`  
  Core data structures:
  - `FrameFeatures` (per-frame multimodal features)
  - `StepEvent` (one step segment)
  - `Procedure` (full task execution)
  - `ActionLabel` (step labels)

- `perception.py`  
  Feature extraction from video and audio, including:
  - optical flow motion and rotation indices
  - MediaPipe Hands features
  - tool and handle heuristic scores
  - audio RMS and onset features

- `context_fsm.py`  
  Ordered workflow FSM that converts `FrameFeatures` into a `Procedure` of `StepEvent` segments, and also computes heuristic confidence scores.

- `llm_socket.py`  
  Instruction generation layer:
  - template instructions (offline)
  - DeepSeek instructions (optional, OpenAI-compatible API, JSON constrained)
  - `process_log.json` export for later instruction pipelines

- `visualization.py`  
  Plots and annotated video:
  - `timeline.png` (Gantt-style step timeline)
  - `features.png` (feature curves with step boundaries)
  - `annotated.mp4` (optional overlay)

- `test_fsm.py`  
  FSM debugging script that prints thresholds, feature ranges, steps, and state transition traces.

## Data in this repository

A sample video file is included in the folder `outputs_llm/`.  
If you want to use the default CLI values without editing scripts, then you can copy the sample video to the repository root, or you can pass the full path with `--video`.

## Installation

### Environment notes
- This project is developed and tested on Windows 10.
- If you install **NumPy >= 2.0**, some libraries can have version conflicts and the program may not run, so I recommend using **NumPy < 2.0**.

### Install with pip
1. Create and activate a virtual environment.
   - Windows (PowerShell):
     ```bash
     python -m venv .venv
     .\.venv\Scripts\Activate.ps1
     ```
   - Windows (cmd):
     ```bash
     python -m venv .venv
     .\.venv\Scripts\activate
     ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

If you want to use DeepSeek instruction generation, then you also need the `openai` Python package, and you need to provide an API key (see the DeepSeek section below).

## How to run

### 1) Run the full pipeline (main.py)

Basic run:

```bash
python main.py --video outputs_llm/20251205_203444.mp4 --output-dir outputs
```

Run and also generate an annotated video:

```bash
python main.py --video outputs_llm/20251205_203444.mp4 --output-dir outputs --annotate-video
```

Run without saving plots:

```bash
python main.py --video outputs_llm/20251205_203444.mp4 --output-dir outputs --no-plots
```

CLI arguments (main ones):

* `--video` or `-v`: path to the input video
* `--task-name`: task name string used in outputs
* `--output-dir` or `-o`: output directory
* `--no-plots`: skip saving PNG plots
* `--annotate-video`: save an annotated video with step labels

### 2) Optional: DeepSeek instruction generation

By default, the system generates template instructions offline.

If you want DeepSeek instructions, you need an API key and the `openai` package, then you can run:

* Windows (PowerShell):

  ```bash
  $env:DEEPSEEK_API_KEY="YOUR_KEY"
  python main.py --video outputs_llm/20251205_203444.mp4 --output-dir outputs --instruction-mode deepseek
  ```

* Windows (cmd):

  ```bash
  set DEEPSEEK_API_KEY=YOUR_KEY
  python main.py --video outputs_llm/20251205_203444.mp4 --output-dir outputs --instruction-mode deepseek
  ```

DeepSeek-related CLI arguments:

* `--instruction-mode deepseek`
* `--deepseek-model` (default `deepseek-chat`)
* `--deepseek-base-url` (default `https://api.deepseek.com`)
* `--deepseek-api-key-env` (default `DEEPSEEK_API_KEY`)
* `--deepseek-temperature`
* `--deepseek-max-tokens`

If the API key is missing, the program prints a message and uses template instructions instead.

### 3) Run FSM debugging (test_fsm.py)

This script is designed for inspection in the terminal, so that you can understand thresholds and why state transitions happened.

Run:

```bash
python test_fsm.py
```

Important note: `test_fsm.py` uses a hardcoded line:

```python
video_path = "20251205_203444.mp4"
```

So you should either edit this path to your video file, or place the video in the repository root with this name.

The debug output includes:

* computed thresholds (motion, rotation, audio, hand-to-center)
* handle-on-table baseline and derived thresholds
* feature ranges for tool and handle scores
* step list with feature summaries and confidences
* committed transitions with reasons
* first candidate transitions (useful when the FSM stays in one state too long)

## Output files

When you run `main.py`, the program writes files into `--output-dir`:

* `features.csv`
  One row per sampled frame, containing all values in `FrameFeatures`, which includes vision motion, hand signals, tool and handle scores, and audio features.

* `steps.csv`
  One row per detected step, including:

  * step id, label, start and end time, duration, confidence, and a stable description key.

* `process_log.json`
  A JSON event log that stores the step list in a stable format, which is designed to be used later as input to an instruction generator or another downstream module.

* `instructions_template.txt` or `instructions_llm.txt`
  Step-by-step instruction document, where the file name depends on the instruction mode.

* `timeline.png`
  A readable step timeline plot (Gantt-style, one line per step).

* `features.png`
  A plot of motion magnitude, rotation index, and audio RMS over time, with step boundaries overlaid.

* `annotated.mp4` (only when `--annotate-video` is used)
  A video copy with step labels overlaid, and optional short instruction text lines if available.

## Design notes (technical solutions used for this assignment)

### Multimodal features (perception.py)

* **Vision motion features**: The system uses dense optical flow (Farneback) and computes motion magnitude, and it computes a rotation index around an estimated lid center, while it also separates the image into a center region and a rim band so that “tightening” and “flipping” patterns can be separated more easily.
* **Hand features**: The system uses MediaPipe Hands to estimate hand landmarks, then it derives hand centroids, hand-to-center distance, per-hand speed, openness and pinch proxies, and palm-normal rotation speed as a simple proxy for wrist turning.
* **Tool and handle heuristics**: The system computes a tool presence score from a top ROI using edge evidence and Hough line segments, and it also computes a handle-on-table score from a top-right ROI based on an adaptive dark pixel ratio.
* **Audio features**: The system extracts audio from the video and computes normalized RMS, plus low and high band RMS and onset signals, because tapping and metallic contact often create useful timing anchors.

### Step inference logic (context_fsm.py)

* The system uses an ordered finite state machine that only progresses forward through the expected assembly procedure, which is useful in this assignment because it produces an explainable step sequence that matches the task structure.
* The FSM uses debouncing and counters for tool presence and for “reach” patterns, because hand tracking and ROI heuristics can be noisy frame by frame.
* The FSM computes per-video thresholds from quantiles of the feature distributions, while it also applies safe minimum values, because a short clip can have weak or missing signals.
* The FSM assigns heuristic confidence scores using the most relevant signals for each step, for example tool score for tool states, and center rotation for hand tightening.

### Optional instruction generation (llm_socket.py)

* Template instructions are deterministic and do not require any external service.
* DeepSeek instructions are optional and use a strict JSON schema, and the code validates that the output step list matches the input event list, so that the instruction output stays aligned with the detected steps.

## Example output (one run)

The following is an example of the generated instruction text from one run:

```text
Task: Pot Lid Assembly
Source video: 20251205_203444.mp4

Automatically derived step by step instructions:

Step 1: LID_REORIENT  (source 2)  (approx. 0.8 s → 3.4 s)
  Reorient the lid so the center hole is accessible.

Step 2: PICK_WASHER  (source 3)  (approx. 3.4 s → 7.8 s)
  Verify: Pick up the washer.

Step 3: SEAT_WASHER  (source 4)  (approx. 7.8 s → 10.8 s)
  Place and press the washer into the center hole.

Step 4: PICK_SCREW  (source 5)  (approx. 10.8 s → 18.6 s)
  Verify: Pick up the screw.

Step 5: INSERT_SCREW  (source 6)  (approx. 18.6 s → 21.4 s)
  Insert and press the screw through the center.

Step 6: PICK_HANDLE  (source 7)  (approx. 21.4 s → 24.2 s)
  Verify: Pick up the handle.

Step 7: INSERT_HANDLE  (source 8)  (approx. 24.2 s → 26.4 s)
  Place and align the handle onto the screw.

Step 8: HAND_TIGHTEN_HANDLE  (source 9)  (approx. 26.4 s → 30.4 s)
  Hand-tighten the handle clockwise.

Step 9: PLACE_LID_FOR_TOOL  (source 10)  (approx. 30.4 s → 31.8 s)
  Flip/place the lid to access the screw head for tool use.

Step 10: PICK_SCREWDRIVER  (source 11)  (approx. 31.8 s → 34.8 s)
  Pick up the screwdriver.

Step 11: TOOL_SEAT  (source 12)  (approx. 34.8 s → 35.8 s)
  Seat the driver tip into the screw head.

Step 12: TOOL_TIGHTEN  (source 13)  (approx. 35.8 s → 40.8 s)
  Tighten the screw using the screwdriver.

Step 13: SET_TOOL_DOWN  (source 14)  (approx. 40.8 s → 42.2 s)
  Verify: Remove and set down the screwdriver.

Step 14: FINAL_FLIP_AND_PLACE  (source 15)  (approx. 42.2 s → 45.6 s)
  Flip and place the assembled lid on the table.

Warnings:
- Do not overtighten the screw to avoid stressing or cracking the glass.

Checks:
- After tightening, gently wiggle the handle to confirm it is firmly attached.
```

## Known limitations

* Only 15 steps are detected.
* The start of `PICK_SCREW` can be marked when the lid is flipping.
* The start and end timestamps of steps are not 100% accurate.
* The annotated video generated is silent.
* If NumPy >= 2.0 is installed, some libraries can have version conflicts and the program may not run.


## Author

Mr Li, MSc student at Aalto University.
